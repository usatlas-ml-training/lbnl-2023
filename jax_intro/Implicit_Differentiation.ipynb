{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implicit Differentiation\n",
        "This notebook will briefly look at implicit differentiation. Implicit differentiation has numerous applications; we will look at a simple fixed point iteration.\n",
        "\n",
        "For a more in-depth guide on its application to machine learning, see the NeurIPS 2020 tutorial on [deep implicit layers](http://implicit-layers-tutorial.org/). Full credit goes to the deep implicit layers tutorial from which we pull examples and applicable code from to provide a primer on the implicit function theorem and itss applications."
      ],
      "metadata": {
        "id": "hWM8j23lCiKR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThAXuiDiCfup"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "from jax import random\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's consider a simple fixed point iteration problem: $z = \\tanh(Wz + x)$. Formally, we wish to solve the problem:\n",
        "\n",
        "Find $z$ such that $g(x, z) = 0$ where $g(x, z) = z - \\tanh(Wz + x)$"
      ],
      "metadata": {
        "id": "6geise9Xn0Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "func = lambda w, x, z: jnp.tanh(W @ z + x)\n",
        "ndim = 10\n",
        "W = random.normal(random.PRNGKey(0), (ndim, ndim)) / jnp.sqrt(ndim)\n",
        "x = random.normal(random.PRNGKey(1), (ndim,)) / jnp.sqrt(ndim)\n",
        "z_init = jnp.zeros_like(x)\n",
        "\n",
        "def fixed_point(func, z_0, tol=1e-5):\n",
        "    z_i = func(z_0)\n",
        "    z_prev = z_0\n",
        "    num_iteration = 1\n",
        "    while jnp.linalg.norm(z_prev - z_i) > tol:\n",
        "        z_prev = z_i\n",
        "        z_i = func(z_i)\n",
        "        num_iteration += 1\n",
        "    return z_i, num_iteration\n",
        "z_star_naive, naive_num_iteration = fixed_point(lambda z: func(W, x, z), z_init)\n",
        "print(naive_num_iteration, 'Fixed point iterations')\n",
        "print(z_star_naive)\n"
      ],
      "metadata": {
        "id": "FGpE4-iXnz_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, we can use Newton's method:\n",
        "$z = z - (\\frac{\\partial g}{\\partial z})^{-1} g(z)$. Since we know the closed form of $g$, we can manually compute $\\frac{\\partial g}{\\partial z}$. However, we can leverage autograd to compute this jacobian for us.\n",
        "\n",
        "$\\frac{\\partial g}{\\partial z} = I - \\text{diag}(\\tanh'(Wz + x))W$."
      ],
      "metadata": {
        "id": "2OLMw4IcoPTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dg_dz(w, x, z):\n",
        "    return jnp.eye(z.shape[0]) - (1 / jnp.cosh(w * z + x) ** 2) * w\n",
        "\n",
        "def newton_solver(func, z_0, tol=1e-5):\n",
        "    func_root = lambda z_i: func(z_i) - z_i\n",
        "    # Using autograd!\n",
        "    newton_eqn = lambda z_i: z_i - jnp.linalg.solve(jax.jacobian(func_root)(z_i), func_root(z_i))\n",
        "    return fixed_point(newton_eqn, z_0, tol=tol)\n",
        "\n",
        "newton_z_star, newton_num_iterations = newton_solver(lambda z: func(W, x, z),\n",
        "                                         z_init)\n",
        "\n",
        "print(\"Difference between newton's method and naive fixed point iteration:\", jnp.linalg.norm(newton_z_star - z_star_naive))\n",
        "print(\"Number of iterations for newton's method:\", newton_num_iterations)\n"
      ],
      "metadata": {
        "id": "8j1Sf59iDhH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far so good! What if we want to find $\\frac{\\partial z^*}{\\partial x}$? This is where we will leverage the **implicit function theorem** (IFT). Before stating the theorem, we'll work through an example:\n",
        "\n",
        "Since this is a fixed point iteration, we know:\n",
        "\n",
        "$\\frac{\\partial g(x, z^*)}{\\partial x} = 0$\n",
        "\n",
        "and expanding via the chain rule:\n",
        "\n",
        "$\\frac{\\partial g(x, z^*)}{\\partial x}  + \\frac{\\partial g(x, z^*)}{\\partial z^*} \\cdot \\frac{\\partial z^*(x)}{\\partial x}= 0$.\n",
        "\n",
        "Thus:\n",
        "\n",
        "$\\frac{\\partial z^*(x)}{\\partial x} = - \\left(\\frac{\\partial g(x, z^*)}{\\partial z^*}\\right) ^{-1} \\cdot  \\frac{\\partial g(x, z^*)}{\\partial x}$.\n",
        "\n",
        "In code:"
      ],
      "metadata": {
        "id": "fkzFPEwRsa6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_dz_dx(func, w, x, z_star):\n",
        "    func_root = lambda z_i: func(w, x, z_i) - z_i\n",
        "    dg_dz_inv = lambda z_i: -1 * jnp.linalg.solve(jax.jacobian(func_root)(z_i), func_root(z_i))\n",
        "    dg_dx = jax.jacobian(func, argnums=1)(w, x, z_star)\n",
        "    return dg_dz_inv(z_star) * dg_dx\n",
        "\n",
        "dz_dx = compute_dz_dx(func, W, x, newton_z_star)\n",
        "\n",
        "\n",
        "def newton_forward(func, w, x, z_init):\n",
        "    return newton_solver(lambda z: func(w, x, z), z_init)[0]\n",
        "\n",
        "dz_dx_autograd = jax.jacobian(newton_forward, argnums=2)(func, W, x, z_init)\n",
        "\n",
        "## Is this actually faster?\n",
        "\n",
        "print('Time for IFT:')\n",
        "%timeit compute_dz_dx(func, W, x, newton_z_star)\n",
        "\n",
        "print('Time for autograd:')\n",
        "%timeit jax.jacobian(newton_forward, argnums=2)(func, W, x, z_init)"
      ],
      "metadata": {
        "id": "1DzLfexLuNiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Differentiating through Newton's method is now orders of magnitude faster!"
      ],
      "metadata": {
        "id": "eKj8W0mlw-Qy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IFT Formally...\n",
        "\n",
        "**Implicit Function Theorem:** Let $f: \\mathbb{R}^p \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ and $a_0 \\in \\mathbb{R}^p$, $z_0 \\in \\mathbb{R}^n$ such that:\n",
        "\n",
        "1. $f(a_0, z_0) = 0$\n",
        "2. $f$ is continuously differentiable with non-singular jacobian $\\partial _1 f(a_0, z_0) \\in \\mathbb{R}^{n\\times n}$.\n",
        "\n",
        "Then there exists an open set $S_{a_0} \\subset \\mathbb{R}^p$, $S_{z_0} \\subset \\mathbb{R}^n$ containing $a_0$ and $z_0$ respectively and a unique continuous function $z^*: S_{a_0} \\rightarrow S_{z_0}$ such that:\n",
        "\n",
        "1. $z_0 = z^*(a_0)$\n",
        "2. $f(a, z^*(a)) = 0 \\forall a \\in S_{a_0}$\n",
        "3. $z^*$ is differentiable on $S_{a_0}$."
      ],
      "metadata": {
        "id": "Mpqod0fYzCWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've covered a simple example of the IFT and shown how the IFT can greatly reduce the cost of differentiating through an optimization loop. However, this notebook only touches the surface of what is possible with the IFT. For some additional references, please see:\n",
        "\n",
        "- [Jaxopt](https://github.com/google/jaxopt) is a great library leveraging the IFT for differentiable optimization problems. They handle the registering of custom gradient definitions in Jax's autograd engine and act as a drop in replacement for many common optimization problems (e.g., constrained QP, root finding). The analogous PyTorch library is [Theseus](https://github.com/facebookresearch/theseus).\n",
        "- [NeurIPS 2020 tutorial](http://implicit-layers-tutorial.org/) on implicit layers."
      ],
      "metadata": {
        "id": "oMt9IJYN3UnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further references:\n",
        "\n"
      ],
      "metadata": {
        "id": "DqdDYUzd1c_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An exercise\n",
        "We leave the following example as an exercise. Consider a similar fixed point iteration problem as Newton solver. However, this time we will use [Anderson Acceleration](https://en.wikipedia.org/wiki/Anderson_acceleration). We provide the implementation of the forward solver; your task is to use the IFT to compute $\\frac{\\partial z^*}{\\partial x}$."
      ],
      "metadata": {
        "id": "HB5Muj2R3Ogi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def anderson_solver(func, z_init, m=5, lam=1e-4, max_iter=50, tol=1e-5, beta=1.0):\n",
        "    x0 = z_init\n",
        "    x1 = func(x0)\n",
        "    x2 = func(x1)\n",
        "    X = jnp.concatenate([jnp.stack([x0, x1]), jnp.zeros((m - 2, *jnp.shape(x0)))])\n",
        "    F = jnp.concatenate([jnp.stack([x1, x2]), jnp.zeros((m - 2, *jnp.shape(x0)))])\n",
        "\n",
        "    res = []\n",
        "    for k in range(2, max_iter):\n",
        "        n = min(k, m)\n",
        "        G = F[:n] - X[:n]\n",
        "        GTG = jnp.tensordot(G, G, [list(range(1, G.ndim))] * 2)\n",
        "        H = jnp.block([[jnp.zeros((1, 1)), jnp.ones((1, n))],\n",
        "                    [ jnp.ones((n, 1)), GTG]]) + lam * jnp.eye(n + 1)\n",
        "        alpha = jnp.linalg.solve(H, jnp.zeros(n+1).at[0].set(1))[1:]\n",
        "\n",
        "        xk = beta * jnp.dot(alpha, F[:n]) + (1-beta) * jnp.dot(alpha, X[:n])\n",
        "        X = X.at[k % m].set(xk)\n",
        "        F = F.at[k % m].set(func(xk))\n",
        "\n",
        "        res = jnp.linalg.norm(F[k % m] - X[k % m]) / (1e-5 + jnp.linalg.norm(F[k % m]))\n",
        "        if res < tol:\n",
        "            break\n",
        "    return xk\n",
        "\n",
        "f = lambda z: func(W, x, z)\n",
        "\n",
        "anderson_solution = anderson_solver(f, z_init)\n",
        "print(anderson_solution)"
      ],
      "metadata": {
        "id": "MbibX_Dw1b-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anderson_solution"
      ],
      "metadata": {
        "id": "OrTMgACJ5V_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZZIsUxb15XE7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}