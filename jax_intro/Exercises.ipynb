{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14413325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695afda0",
   "metadata": {},
   "source": [
    "## VMAP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775ec51",
   "metadata": {},
   "source": [
    "`gaussian` is the density of a normal distribution. We take its derivative, so implicit vectorization does not apply. Apply `jax.vmap` correctly, so that the code below works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, params):\n",
    "    mu, sigma = params\n",
    "    a = ((x - mu) / sigma) ** 2\n",
    "    b = sigma * jnp.sqrt(2 * jnp.pi)\n",
    "    return jnp.exp(-a / 2) / b\n",
    "\n",
    "\n",
    "dgaussian = jax.grad(gaussian)\n",
    "\n",
    "params = jnp.array([0.2, 2.0])\n",
    "\n",
    "# generate a random vector and matrix\n",
    "key = jrandom.PRNGKey(42)\n",
    "key, subkey_1, subkey_2 = jrandom.split(key, 3)\n",
    "xs_vec = jrandom.uniform(subkey_1, (20,))\n",
    "xs_matr = jrandom.uniform(subkey_2, (30, 30))\n",
    "\n",
    "# TODO: define dgaussian_vec and dgaussian_matr to accept vectors and matrices\n",
    "dgaussian_vec = dgaussian\n",
    "dgaussian_matr = dgaussian\n",
    "\n",
    "ys_vec = dgaussian_vec(xs_vec, params)\n",
    "ys_matr = dgaussian_matr(xs_matr, params)\n",
    "\n",
    "print(ys_vec.shape, ys_matr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be671fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "m_1 = jnp.array([[1.0, 0.0], [0.0, 100.0]])\n",
    "m_2 = jnp.array([[1.0, 1.0], [-1.0, 1.0]])\n",
    "\n",
    "\n",
    "def cond_plus_cond_sq(m):\n",
    "    c = jnp.linalg.cond(m)\n",
    "    return c + c**2\n",
    "\n",
    "\n",
    "m = jnp.array((m_1, m_2))\n",
    "\n",
    "# Question: how to replace this for-loop with jax.vmap?\n",
    "cs = []\n",
    "for i in range(m.shape[2]):\n",
    "    x = m[:, :, i].reshape(2, 2)\n",
    "    c = jnp.linalg.cond(x)\n",
    "    cs.append(c)\n",
    "cs = jnp.array(cs)\n",
    "\n",
    "# NB: jax.vmap by default does something different:\n",
    "print(jax.vmap(cond_plus_cond_sq)(m))\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1e929",
   "metadata": {},
   "source": [
    "## Cubic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f5f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "true_a = -0.5\n",
    "true_b = 2.3\n",
    "true_c = 1.0\n",
    "true_d = -0.2\n",
    "\n",
    "true_params = jnp.array((true_a, true_b, true_c, true_d))\n",
    "\n",
    "\n",
    "# TODO-1\n",
    "@jax.jit\n",
    "def predict(params, x):\n",
    "    # interpret params as coefficents a, b, c, d\n",
    "    # return ax^4 + bx^3 + cx^2 + d\n",
    "    pass\n",
    "\n",
    "\n",
    "# generate noisy data: first, evaluate the true cubic polynomial on 200 points\n",
    "xs = jnp.linspace(-2.0, 2.0, 200)\n",
    "predict_v = jax.vmap(predict, in_axes=(None, 0))\n",
    "ys = predict_v(true_params, xs)\n",
    "\n",
    "# then add some random noise\n",
    "eps = 0.2\n",
    "seed = 2\n",
    "noise = jrandom.uniform(jrandom.PRNGKey(seed), ys.shape, minval=-eps, maxval=eps)\n",
    "\n",
    "noisy_ys = ys + noise\n",
    "\n",
    "\n",
    "# TODO-2\n",
    "@jax.jit\n",
    "def loss(params, xs, ys):\n",
    "    # return MSE loss: \\frac{1}{n} \\sum (predicted[i] - ys[i])^2\n",
    "    pass\n",
    "\n",
    "\n",
    "params = jrandom.uniform(jrandom.PRNGKey(seed), (4,))\n",
    "\n",
    "n_steps = 200\n",
    "lr = 0.05\n",
    "\n",
    "loss_and_grad = jax.jit(jax.value_and_grad(loss))\n",
    "\n",
    "for step in range(n_steps):\n",
    "    curr_loss, params_grad = loss_and_grad(params, xs, noisy_ys)\n",
    "\n",
    "    params = params - lr * params_grad\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, loss = {curr_loss}, {params = }\")\n",
    "        plt.plot(xs, predict_v(params, xs), color=\"red\")\n",
    "        plt.plot(xs, noisy_ys, color=\"green\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebddbbf5",
   "metadata": {},
   "source": [
    "## JIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c005cd",
   "metadata": {},
   "source": [
    "How many times will `jax.jit` compile `f` in the following example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9966d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_enable_x64\", False)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x):\n",
    "    print(\"Compiling: \", x)\n",
    "    return x - jnp.sum(x)\n",
    "\n",
    "\n",
    "x1 = jnp.ones((2,))\n",
    "x2 = jnp.zeros((2,))\n",
    "x3 = jnp.ones((3,))\n",
    "x4 = jnp.ones((2, 2))\n",
    "x5 = jnp.ones((4, 4))\n",
    "x6 = jnp.ones((3, 3, 3))\n",
    "x7 = jnp.ones((2,), dtype=jnp.int32)\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "x8 = jnp.ones((2,), dtype=jnp.float64)\n",
    "\n",
    "for x in [x1, x2, x3, x4, x5, x6, x7, x8]:\n",
    "    print(\"f(x) = \", f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e35353",
   "metadata": {},
   "source": [
    "## Legendre\n",
    "\n",
    "1. For `legendre_1`, use recursive formula $P_n(x) = \\frac{(2n-1)xP_{n-1}(x) - (n-1) P_{n-2}(x)}{n}$.\n",
    "2. For `legendre_2`, use formula $P_n(x) = \\frac{1}{2^n n!} \\frac{d^n}{dx^n} (x^2 - 1)^n$.\n",
    "3. Fix `jax.jit` calls so that they work.\n",
    "4. Time different combinations of `jax.jit` and `jax.vmap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff75b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Legendre polynomial P_n(x) using recursive formula\n",
    "# P_n(x) = \\frac{(2n-1)xP_{n-1}(x) - (n-1) P_{n-2}(x)}{n}\n",
    "def legendre_1(x, n):\n",
    "    return None\n",
    "\n",
    "\n",
    "# Compute Legendre polynomial P_n(x) using formula\n",
    "# P_n(x) = \\frac{1}{2^n n!} \\frac{d^n}{dx^n} (x^2 - 1)^n\n",
    "\n",
    "\n",
    "def legendre_2(x, n):\n",
    "    helper = lambda y: (y**2 - 1) ** n\n",
    "    return None\n",
    "\n",
    "\n",
    "# This function is to check previous two\n",
    "def legendre_3(x, n):\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    elif n == 1:\n",
    "        return x\n",
    "    elif n == 2:\n",
    "        return (3 * x**2 - 1) / 2\n",
    "    elif n == 3:\n",
    "        return (5 * x**3 - 3 * x) / 2\n",
    "    elif n == 4:\n",
    "        return (35 * x**4 - 30 * x * x + 3) / 8\n",
    "    else:\n",
    "        raise RuntimeError(\"not implemented\")\n",
    "\n",
    "\n",
    "# what can be jit-ted? How to fix the jax.jit call?\n",
    "jlegendre_1 = jax.jit(legendre_1)\n",
    "jlegendre_2 = jax.jit(legendre_2)\n",
    "jlegendre_3 = jax.jit(legendre_3)\n",
    "\n",
    "\n",
    "x = 0.3\n",
    "n = 4\n",
    "\n",
    "print(jlegendre_1(x, n))\n",
    "print(jlegendre_2(x, n))\n",
    "print(jlegendre_3(x, n))\n",
    "\n",
    "# Timing\n",
    "xs = np.linspace(0, 4, 100000)\n",
    "zs = np.zeros_like(xs)\n",
    "\n",
    "# Profile the combination of jit and vmap. What is faster: vmap after jit\n",
    "# or jit after vmap?\n",
    "\n",
    "# Evaluate transformed function on zs once to pre-compile it.\n",
    "# Evaluate transformed function on xs to measure the runtime.\n",
    "xs = np.linspace(0, 4, 100000)\n",
    "zs = np.zeros_like(xs)\n",
    "\n",
    "\n",
    "for f in [legendre_1, legendre_2, legendre_3]:\n",
    "    # apply jit first, then vmap\n",
    "    f_jv = None\n",
    "    # call f_jv once on zs, once on xs, measuring the time\n",
    "    # of the second call\n",
    "\n",
    "\n",
    "for f in [legendre_1, legendre_2, legendre_3]:\n",
    "    # apply vmap first, then jit\n",
    "    f_vj = None\n",
    "    # call f_jv once on zs, once on xs, measuring the time\n",
    "    # of the second call"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
